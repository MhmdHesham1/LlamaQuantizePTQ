# -*- coding: utf-8 -*-
"""LlamaQuantizePTQ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mcultXErBxXdX-a8qDR3qhtjQ2AYw7KN
"""

!pip install transformers accelerate bitsandbytes # transformer is the core library in HF to access transformers models - bits and bytes for quantization - accelration for optimization

from transformers import AutoTokenizer, AutoModelForCausalLM
model_name = 'meta-llama/Llama-2-7b-chat-hf'
prompt ='tell me about gravity'
access_token='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'

model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map='auto', use_auth_token=access_token)# device map tell the model gpu or cpu - load in 4 bit to quantize in 4 bit
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token) # to fetch the model tokenizer

model_input = tokenizer(prompt, return_tensors='pt').to("cuda:0") # to tokenize the input
output=model.generate(**model_input)
print(tokenizer.decode(output[0], skip_special_tokens=True))

